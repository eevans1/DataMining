---
title: "Exercise 2"
output: github_document
date: "`r format(Sys.time(), '%d %B %Y')`"
---

Wyatt Allen, Elijah Evans, David Ford, Patrick Scovel

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
## Question 1 Updated
library(FNN)
library(mosaic)

# Create an array of lm models
data(SaratogaHouses)

# Make variables numeric
SaratogaHouses$heatingElectric <- ifelse(SaratogaHouses$heating == "electric", 1, 0)
SaratogaHouses$heatingWater <- ifelse(SaratogaHouses$heating == "hot water/steam", 1, 0)
SaratogaHouses$heatingAir <- ifelse(SaratogaHouses$heating == "hot air", 1, 0)

SaratogaHouses$fuelGas <- ifelse(SaratogaHouses$fuel == "gas", 1, 0)
SaratogaHouses$fuelElectric <- ifelse(SaratogaHouses$fuel == "electric", 1, 0)
SaratogaHouses$fuelOil <- ifelse(SaratogaHouses$fuel == "oil", 1, 0)

SaratogaHouses$sewerSeptic <- ifelse(SaratogaHouses$sewer == "septic", 1, 0)
SaratogaHouses$sewerPublic <- ifelse(SaratogaHouses$sewer == "public/commercial", 1, 0)
SaratogaHouses$sewerNone <- ifelse(SaratogaHouses$sewer == "none", 1, 0)

SaratogaHouses$waterfront <- ifelse(SaratogaHouses$waterfront == "Yes", 1, 0)
SaratogaHouses$newConstruction <- ifelse(SaratogaHouses$newConstruction == "Yes", 1, 0)
SaratogaHouses$centralAir <- ifelse(SaratogaHouses$centralAir == "Yes", 1, 0)

# Model formula
formula <- as.formula(price ~ 
                        newConstruction +
                        lotSize + 
                        landValue + 
                        livingArea + 
                        bedrooms +
                        bathrooms +
                        rooms +
                        heating +
                        waterfront +
                        centralAir)

# OLS
lm = lm(formula, data=SaratogaHouses)
summary(lm)

## Objective function
rmse = function(y, yhat) {
  sqrt(mean((y - yhat)^2 ))
}

# Dataframe of RMSE values
rmse_df <- data.frame()

for(i in 1:100){
  # Split into training and testing sets
  n = nrow(SaratogaHouses)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  saratoga_train = SaratogaHouses[train_cases,]
  saratoga_test = SaratogaHouses[test_cases,]
  
  ## Linear model
  lm_train = lm(formula, data=saratoga_train)
  summary(lm_train) 
  
  yhat_test = predict(lm_train, saratoga_test)
  
  # Root mean-squared prediction error
  rmse_df <- rbind(rmse_df, c(i, rmse(saratoga_test$price, yhat_test)))
} 
colnames(rmse_df) <- c("i", "rmse")
rmse_lm <- mean(rmse_df$rmse)
rmse_lm

########### KNN ############

# Question 1 Modified
library(FNN)
# Create an array of lm models
data(SaratogaHouses)
## Objective function
rmse = function(y, yhat) {
  sqrt(mean((y - yhat)^2 ))
}


# Dataframe of RMSE values
rmse_df <- data.frame()
knn_df <- data.frame(matrix(ncol = 2, nrow = 0))

# select a training set
n = nrow(SaratogaHouses)
n_train = round(0.8*n)
n_test = n - n_train

for(i in 2:100){
  for(j in 1:100) {
    train_cases = sample.int(n, n_train, replace=FALSE)
    test_cases = setdiff(1:n, train_cases)
    saratoga_train = SaratogaHouses[train_cases,]
    saratoga_test = SaratogaHouses[test_cases,]
    
    # construct the training and test-set feature matrices
    # note the "-1": this says "don't add a column of ones for the intercept"
    Xtrain = model.matrix(~ newConstruction +
                            lotSize + 
                            landValue + 
                            livingArea + 
                            bedrooms +
                            bathrooms +
                            rooms +
                            heating +
                            waterfront +
                            centralAir - 1, data=saratoga_train)
    Xtest = model.matrix(~ newConstruction +
                           lotSize + 
                           landValue + 
                           livingArea + 
                           bedrooms +
                           bathrooms +
                           rooms +
                           heating +
                           waterfront +
                           centralAir - 1, data=saratoga_test)
    
    # training and testing set responses
    ytrain = saratoga_train$price
    ytest = saratoga_test$price
    
    # now rescale:
    scale_train = apply(Xtrain, 2, sd)  # calculate std dev for each column
    Xtilde_train = scale(Xtrain, scale = scale_train) 
    Xtilde_test = scale(Xtest, scale = scale_train)  # use the training set scales!
    
    ## KNN regression model
    knn_model = knn.reg(Xtilde_train, Xtilde_test, ytrain, k=i)
    rmse_knn <- rmse(ytest, knn_model$pred)
    
    ## Store all stastics in df
    knn_df <- rbind(knn_df, list(i, rmse_knn))
  }
}
colnames(knn_df) <- c("knn","rmse")
rmse_df <- aggregate(rmse ~ knn, knn_df, mean)

rmse_df[which.min(rmse_df$rmse),]
rmse_lm

## Below is our graph for RMSE across many values of K
# The Red line represents the RMSE of a linear model, 
# The minimum RMSE of the linear model which is typically around 58,000
# The Green line represnt the minimum RMSE of the KNN model, 
# The minimum K value is usually between 10 and 20 with a RMSE of around 61,000

plot(rmse ~ knn, data = rmse_df, 
     ylim=c(rmse_lm-rmse_lm*0.03, max(rmse_df$rmse) + max(rmse_df$rmse)*0.03), 
     main= "Comparing RMSE between KNN and a Linear Model")
abline(h=rmse_lm, col = 2)
abline(v=which.min(rmse_df$rmse), col = 3)




##Question 2
# Libraries, idk which ones we really need??
library(tidyverse)
library(foreach)
library(FNN)

# Reading in the raw data from online
brca <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/brca.csv", header=TRUE)

# Overall cancer rate + defining variables
n=nrow(brca)
cancer_rate <- sum(brca$cancer==1)/n #the total rate of breast cancer
y=brca$cancer
threshold = 0.1 #threshold for prediction models, NEED TO FIND GOOD ONE
rmse = function(y,yhat) {
  sqrt(mean((y - yhat)^2))
}

# creating the train test split

coef_df <- data.frame()

for (i in 1:5000){
  n_train <- round(0.8*n)
  n_test <- n - n_train
  train_pats <-sample.int(n,n_train,replace=FALSE)
  test_pats <- setdiff(1:n, train_pats)
  brca_train <- brca[train_pats,]
  brca_test <- brca[test_pats,]
  
  
  #running logit regression for recall on all other variables for train set
  
  logit_drcon = glm(recall ~ ., data=brca_train, family='binomial')
  coef_df <- rbind(coef_df, c(i,coef(logit_drcon)))
  
}

colnames(coef_df) <- c("i", "intercept", "radiologist34", "radiologist66", "radiologist89", "radiologist95",
                       "cancer", "age5059", "age6069", "age70plus", "history", "symptoms", "menopausepostmenoNoHT", "menopausepostmenounknown", "menopausepremeno",
                       "densitydensity2","densitydensity3","densitydensity4")
means_df <- colMeans(coef_df)
means_df

mean(coef_df$radiologist34)
mean(coef_df$radiologist66)
mean(coef_df$radiologist89)
mean(coef_df$radiologist95)

# Out-of-sample accuracy
logit_drcon_test_pred = predict(logit_drcon, brca_test)
logit_drcon_test_yhat = ifelse(logit_drcon_test_pred > threshold, 1, 0) #NEED TO ISOLATE GOOD THRESHOLD!
logit_drcon_confusion_out = table(y=brca_test$cancer, yhat=logit_drcon_test_yhat)
logit_drcon_test_accuracy = sum(diag(logit_drcon_confusion_out))/sum(logit_drcon_confusion_out) #checking accuracy


# Question 2: Are doctors appropriately weighing risk factors?

#first lets run a logit regression of cancer on all risk factors, including recall to get some 
#baseline effects


logit_all = glm(cancer ~ . , data=brca_train, family="binomial")
logit_all

#as we can see, pretty much everything seems to be having some effect, other than maybe being pre-menopausal


# Consider a logit regression of cancer on just recall
#first reset our threshold so that our logit model will have some predictions of cancer = 1

threshold = -2.5 #threshold for prediction model

# now creating a new dataframe so that we can store our results of replicated test train splits
# then we are looping 5000 times a train/test split, storing prediction accuracies, then displaying the mean
coef_recall <- data.frame()

for (i in 1:5000){
  n_train <- round(0.8*n)
  n_test <- n - n_train
  train_pats <-sample.int(n,n_train,replace=FALSE)
  test_pats <- setdiff(1:n, train_pats)
  brca_train <- brca[train_pats,]
  brca_test <- brca[test_pats,]
  
  # logit model
  
  logit_recall = glm(cancer ~ recall, data=brca_train, family="binomial")
  logit_recall
  
  #out of sample accuracy
  
  logit_recall_test_pred = predict(logit_recall, brca_test)
  logit_recall_test_yhat = ifelse(logit_recall_test_pred > threshold, 1, 0) #NEED TO ISOLATE GOOD THRESHOLD!
  logit_recall_confusion_out = table(y=brca_test$cancer, yhat=logit_recall_test_yhat)
  logit_recall_confusion_out
  logit_recall_test_accuracy = sum(diag(logit_recall_confusion_out))/sum(logit_recall_confusion_out) #checking accuracy
  logit_recall_test_accuracy
  
  coef_recall <- rbind(coef_recall, c(i, coef(logit_recall), logit_recall_test_accuracy))
}

colnames(coef_recall) <- c("i", "intercept", "recall", "accuracy")
means_recall <- colMeans(coef_recall)
means_recall


#this gives us a baseline result of roughly 85.72% accuracy

# now that we have a baseline, we can start to look at logit regressions of cancer on recall + 
# other individual risk factors
# first we'll look at density, since it seemed fairly important in the first model
# looping this model 5000 times, similarly to above

coef_density <- data.frame()

for (i in 1:5000){
  n_train <- round(0.8*n)
  n_test <- n - n_train
  train_pats <-sample.int(n,n_train,replace=FALSE)
  test_pats <- setdiff(1:n, train_pats)
  brca_train <- brca[train_pats,]
  brca_test <- brca[test_pats,]
  
  
  logit_plus_density = glm(cancer ~ recall + density, data=brca_train, family='binomial')
  
  #out of sample accuracy
  
  logit_plus_density_test_pred = predict(logit_plus_density, brca_test)
  logit_plus_density_test_yhat = ifelse(logit_plus_density_test_pred > threshold, 1, 0) #NEED TO ISOLATE GOOD THRESHOLD!
  logit_plus_density_confusion_out = table(y=brca_test$cancer, yhat=logit_plus_density_test_yhat)
  logit_plus_density_confusion_out
  logit_plus_density_test_accuracy = sum(diag(logit_plus_density_confusion_out))/sum(logit_plus_density_confusion_out) #checking accuracy
  logit_plus_density_test_accuracy
  
  
  coef_density <- rbind(coef_density, c(i, coef(logit_plus_density), logit_plus_density_test_accuracy))
  
  
  
}

colnames(coef_density) <- c("i", "intercept", "recall", "density1", "density2", "density3", "accuracy")
means_density <- colMeans(coef_density)
means_density

#it seems that dr.'s do a pretty good job of taking density into account, which makes sense becuase it is 
# a very powerful indicator of cancer

#let's try menopause
#another train/test split

coef_meno <- data.frame()

for (i in 1:5000){
  n_train <- round(0.8*n)
  n_test <- n - n_train
  train_pats <-sample.int(n,n_train,replace=FALSE)
  test_pats <- setdiff(1:n, train_pats)
  brca_train <- brca[train_pats,]
  brca_test <- brca[test_pats,]
  
  
  logit_plus_meno = glm(cancer ~ recall + menopause, data=brca_train, family='binomial')
  
  #out of sample accuracy
  
  logit_plus_meno_test_pred = predict(logit_plus_meno, brca_test)
  logit_plus_meno_test_yhat = ifelse(logit_plus_meno_test_pred > threshold, 1, 0) #NEED TO ISOLATE GOOD THRESHOLD!
  logit_plus_meno_confusion_out = table(y=brca_test$cancer, yhat=logit_plus_meno_test_yhat)
  logit_plus_meno_confusion_out
  logit_plus_meno_test_accuracy = sum(diag(logit_plus_meno_confusion_out))/sum(logit_plus_meno_confusion_out) #checking accuracy
  logit_plus_meno_test_accuracy
  
  
  coef_meno <- rbind(coef_meno, c(i, coef(logit_plus_meno), logit_plus_meno_test_accuracy))
  
  
  
}

colnames(coef_meno) <- c("i", "intercept", "recall", "menopausepostmenoNoHT", "menopausepostmenounknown", "menopausepremeno", "accuracy")
means_meno <- colMeans(coef_meno)
means_meno

#menopause seems to be accounted for pretty well, lets try age

# same loop as before but with age
coef_age <- data.frame()

for (i in 1:5000){
  n_train <- round(0.8*n)
  n_test <- n - n_train
  train_pats <-sample.int(n,n_train,replace=FALSE)
  test_pats <- setdiff(1:n, train_pats)
  brca_train <- brca[train_pats,]
  brca_test <- brca[test_pats,]
  
  logit_plus_age = glm(cancer ~ recall + age, data=brca_train, family="binomial")
  logit_plus_age
  
  
  
  #out of sample accuracy
  
  logit_plus_age_test_pred = predict(logit_plus_age, brca_test)
  logit_plus_age_test_yhat = ifelse(logit_plus_age_test_pred > threshold, 1, 0) #NEED TO ISOLATE GOOD THRESHOLD!
  logit_plus_age_confusion_out = table(y=brca_test$cancer, yhat=logit_plus_age_test_yhat)
  logit_plus_age_confusion_out
  logit_plus_age_test_accuracy = sum(diag(logit_plus_age_confusion_out))/sum(logit_plus_age_confusion_out) #checking accuracy
  logit_plus_age_test_accuracy
  
  coef_age <- rbind(coef_age, c(i, coef(logit_plus_age), logit_plus_age_test_accuracy))
  }

colnames(coef_age) <- c("i", "intercept", "recall", "40s", "50s", "60s", "accuracy")
means_age <- colMeans(coef_age)
means_age


#age also seems to be accounted for, lets try family history  

coef_fhist <- data.frame()

for (i in 1:5000){
  n_train <- round(0.8*n)
  n_test <- n - n_train
  train_pats <-sample.int(n,n_train,replace=FALSE)
  test_pats <- setdiff(1:n, train_pats)
  brca_train <- brca[train_pats,]
  brca_test <- brca[test_pats,]
  
  logit_plus_fhist = glm(cancer ~ recall + history, data=brca_train, family="binomial")
  logit_plus_fhist
  
  #out of sample accuracy
  
  logit_plus_fhist_test_pred = predict(logit_plus_fhist, brca_test)
  logit_plus_fhist_test_yhat = ifelse(logit_plus_fhist_test_pred > threshold, 1, 0) #NEED TO ISOLATE GOOD THRESHOLD!
  logit_plus_fhist_confusion_out = table(y=brca_test$cancer, yhat=logit_plus_fhist_test_yhat)
  logit_plus_fhist_confusion_out
  logit_plus_fhist_test_accuracy = sum(diag(logit_plus_fhist_confusion_out))/sum(logit_plus_fhist_confusion_out) #checking accuracy
  logit_plus_fhist_test_accuracy
  
  coef_fhist <- rbind(coef_fhist, c(i, coef(logit_plus_fhist), logit_plus_fhist_test_accuracy))
  
}

colnames(coef_fhist) <- c("i", "intercept", "recall", "fhist", "accuracy")
means_fhist <- colMeans(coef_fhist)
means_fhist

#it looks like dr.s are also really good at taking into account history
#lets examine mamm

coef_symp <- data.frame()

for (i in 1:5000){
  n_train <- round(0.8*n)
  n_test <- n - n_train
  train_pats <-sample.int(n,n_train,replace=FALSE)
  test_pats <- setdiff(1:n, train_pats)
  brca_train <- brca[train_pats,]
  brca_test <- brca[test_pats,]
  
  logit_plus_symp = glm(cancer ~ recall + history, data=brca_train, family="binomial")
  logit_plus_symp
  
  #out of sample accuracy
  
  logit_plus_symp_test_pred = predict(logit_plus_symp, brca_test)
  logit_plus_symp_test_yhat = ifelse(logit_plus_symp_test_pred > threshold, 1, 0) #NEED TO ISOLATE GOOD THRESHOLD!
  logit_plus_symp_confusion_out = table(y=brca_test$cancer, yhat=logit_plus_symp_test_yhat)
  logit_plus_symp_confusion_out
  logit_plus_symp_test_accuracy = sum(diag(logit_plus_symp_confusion_out))/sum(logit_plus_symp_confusion_out) #checking accuracy
  logit_plus_symp_test_accuracy
  
  coef_symp <- rbind(coef_symp, c(i, coef(logit_plus_symp), logit_plus_symp_test_accuracy))
  
}

colnames(coef_symp) <- c("i", "intercept", "recall", "Symptoms", "accuracy")
means_symp <- colMeans(coef_symp)
means_symp

#again, it appears that the dr's, in general, are 
#lets look at a few pairs of risk factors
#here we are looking at a logit regression of cancer on recall + age + density
coef_12 <- data.frame()

for (i in 1:5000){
  n_train <- round(0.8*n)
  n_test <- n - n_train
  train_pats <-sample.int(n,n_train,replace=FALSE)
  test_pats <- setdiff(1:n, train_pats)
  brca_train <- brca[train_pats,]
  brca_test <- brca[test_pats,]
  
  logit_plus_12 = glm(cancer ~ recall + age + density, data=brca_train, family="binomial")
  logit_plus_12
  
  #out of sample accuracy
  
  logit_plus_12_test_pred = predict(logit_plus_12, brca_test)
  logit_plus_12_test_yhat = ifelse(logit_plus_12_test_pred > threshold, 1, 0) #NEED TO ISOLATE GOOD THRESHOLD!
  logit_plus_12_confusion_out = table(y=brca_test$cancer, yhat=logit_plus_12_test_yhat)
  logit_plus_12_confusion_out
  logit_plus_12_test_accuracy = sum(diag(logit_plus_12_confusion_out))/sum(logit_plus_12_confusion_out) #checking accuracy
  logit_plus_12_test_accuracy
  
  coef_12 <- rbind(coef_12, c(i, coef(logit_plus_12), logit_plus_12_test_accuracy))
  
  
  
}

colnames(coef_12) <- c("i", "intercept", "recall", "40", "50", "60", "density1", "density2", "density3", "accuracy")
means_12 <- colMeans(coef_12)
means_12

#this actualy appears to be a more accurate model, suggesting that, while individually these risk
#factors are not significant, they are jointly significant.

#here lets look at the means for all of the different factors, as well as the accuracy rates for each model

means_recall
means_age
means_meno
means_symp
means_fhist
means_12
```

Question 1

Below is our graph for RMSE across many values of K. The Red line represents the RMSE of a linear model; the minimum RMSE of the linear model which is typically around 58,000. The Green line represnt the minimum RMSE of the KNN model. The minimum K value is usually between 10 and 20 with a RMSE of around 61,000

```{r echo=FALSE}
plot(rmse ~ knn, data = rmse_df, 
     ylim=c(rmse_lm-rmse_lm*0.03, max(rmse_df$rmse) + max(rmse_df$rmse)*0.03), 
     main= "Comparing RMSE between KNN and a Linear Model")
abline(h=rmse_lm, col = 2)
abline(v=which.min(rmse_df$rmse), col = 3)
```