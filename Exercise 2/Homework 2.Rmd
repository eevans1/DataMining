---
title: "Exercise 2"
output: github_document
date: "`r format(Sys.time(), '%d %B %Y')`"
---

Wyatt Allen, Elijah Evans, David Ford, Patrick Scovel

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
## Question 1 Updated
library(FNN)
library(mosaic)

# Create an array of lm models
data(SaratogaHouses)

# Make variables numeric
SaratogaHouses$heatingElectric <- ifelse(SaratogaHouses$heating == "electric", 1, 0)
SaratogaHouses$heatingWater <- ifelse(SaratogaHouses$heating == "hot water/steam", 1, 0)
SaratogaHouses$heatingAir <- ifelse(SaratogaHouses$heating == "hot air", 1, 0)

SaratogaHouses$fuelGas <- ifelse(SaratogaHouses$fuel == "gas", 1, 0)
SaratogaHouses$fuelElectric <- ifelse(SaratogaHouses$fuel == "electric", 1, 0)
SaratogaHouses$fuelOil <- ifelse(SaratogaHouses$fuel == "oil", 1, 0)

SaratogaHouses$sewerSeptic <- ifelse(SaratogaHouses$sewer == "septic", 1, 0)
SaratogaHouses$sewerPublic <- ifelse(SaratogaHouses$sewer == "public/commercial", 1, 0)
SaratogaHouses$sewerNone <- ifelse(SaratogaHouses$sewer == "none", 1, 0)

SaratogaHouses$waterfront <- ifelse(SaratogaHouses$waterfront == "Yes", 1, 0)
SaratogaHouses$newConstruction <- ifelse(SaratogaHouses$newConstruction == "Yes", 1, 0)
SaratogaHouses$centralAir <- ifelse(SaratogaHouses$centralAir == "Yes", 1, 0)

# Model formula
formula <- as.formula(price ~ 
                        newConstruction +
                        lotSize + 
                        landValue + 
                        livingArea + 
                        bedrooms +
                        bathrooms +
                        rooms +
                        heating +
                        waterfront +
                        centralAir)

# OLS
lm = lm(formula, data=SaratogaHouses)
summary(lm)

## Objective function
rmse = function(y, yhat) {
  sqrt(mean((y - yhat)^2 ))
}

# Dataframe of RMSE values
rmse_df <- data.frame()

for(i in 1:100){
  # Split into training and testing sets
  n = nrow(SaratogaHouses)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  saratoga_train = SaratogaHouses[train_cases,]
  saratoga_test = SaratogaHouses[test_cases,]
  
  ## Linear model
  lm_train = lm(formula, data=saratoga_train)
  summary(lm_train) 
  
  yhat_test = predict(lm_train, saratoga_test)
  
  # Root mean-squared prediction error
  rmse_df <- rbind(rmse_df, c(i, rmse(saratoga_test$price, yhat_test)))
} 
colnames(rmse_df) <- c("i", "rmse")
rmse_lm <- mean(rmse_df$rmse)
rmse_lm

########### KNN ############

# Question 1 Modified
library(FNN)
# Create an array of lm models
data(SaratogaHouses)
## Objective function
rmse = function(y, yhat) {
  sqrt(mean((y - yhat)^2 ))
}


# Dataframe of RMSE values
rmse_df <- data.frame()
knn_df <- data.frame(matrix(ncol = 2, nrow = 0))

# select a training set
n = nrow(SaratogaHouses)
n_train = round(0.8*n)
n_test = n - n_train

for(i in 2:100){
  for(j in 1:100) {
    train_cases = sample.int(n, n_train, replace=FALSE)
    test_cases = setdiff(1:n, train_cases)
    saratoga_train = SaratogaHouses[train_cases,]
    saratoga_test = SaratogaHouses[test_cases,]
    
    # construct the training and test-set feature matrices
    # note the "-1": this says "don't add a column of ones for the intercept"
    Xtrain = model.matrix(~ newConstruction +
                            lotSize + 
                            landValue + 
                            livingArea + 
                            bedrooms +
                            bathrooms +
                            rooms +
                            heating +
                            waterfront +
                            centralAir - 1, data=saratoga_train)
    Xtest = model.matrix(~ newConstruction +
                           lotSize + 
                           landValue + 
                           livingArea + 
                           bedrooms +
                           bathrooms +
                           rooms +
                           heating +
                           waterfront +
                           centralAir - 1, data=saratoga_test)
    
    # training and testing set responses
    ytrain = saratoga_train$price
    ytest = saratoga_test$price
    
    # now rescale:
    scale_train = apply(Xtrain, 2, sd)  # calculate std dev for each column
    Xtilde_train = scale(Xtrain, scale = scale_train) 
    Xtilde_test = scale(Xtest, scale = scale_train)  # use the training set scales!
    
    ## KNN regression model
    knn_model = knn.reg(Xtilde_train, Xtilde_test, ytrain, k=i)
    rmse_knn <- rmse(ytest, knn_model$pred)
    
    ## Store all stastics in df
    knn_df <- rbind(knn_df, list(i, rmse_knn))
  }
}
colnames(knn_df) <- c("knn","rmse")
rmse_df <- aggregate(rmse ~ knn, knn_df, mean)

rmse_df[which.min(rmse_df$rmse),]
rmse_lm

## Below is our graph for RMSE across many values of K
# The Red line represents the RMSE of a linear model, 
# The minimum RMSE of the linear model which is typically around 58,000
# The Green line represnt the minimum RMSE of the KNN model, 
# The minimum K value is usually between 10 and 20 with a RMSE of around 61,000

plot(rmse ~ knn, data = rmse_df, 
     ylim=c(rmse_lm-rmse_lm*0.03, max(rmse_df$rmse) + max(rmse_df$rmse)*0.03), 
     main= "Comparing RMSE between KNN and a Linear Model")
abline(h=rmse_lm, col = 2)
abline(v=which.min(rmse_df$rmse), col = 3)
```

Question 1

Below is our graph for RMSE across many values of K. The Red line represents the RMSE of a linear model; the minimum RMSE of the linear model which is typically around 58,000. The Green line represnt the minimum RMSE of the KNN model. The minimum K value is usually between 10 and 20 with a RMSE of around 61,000

```{r echo=FALSE}
plot(rmse ~ knn, data = rmse_df, 
     ylim=c(rmse_lm-rmse_lm*0.03, max(rmse_df$rmse) + max(rmse_df$rmse)*0.03), 
     main= "Comparing RMSE between KNN and a Linear Model")
abline(h=rmse_lm, col = 2)
abline(v=which.min(rmse_df$rmse)+1, col = 3)
```

Question 2
To answer the first question, we decided to take a very direct approach. We started by dividing the data set into a train/test split. We then ran a logit regression of recall on dummy variables for four of the individual doctors, as well as the other risk factors using only the training set. Using the estimated coefficients, we then tested our results on the test set, and saw an accuracy rate of >99%. This seems like a very strong indicator that the logit model is a good estimator of reality. Now we can determine individual doctor effect, simply by looking at their coefficient, while remembering that the baseline is the estimate for Dr. 13. 
-Dr. 89: ~ 0.48
-Dr. 66: ~ 0.41
-Dr. 95: ~0.03
-Dr. 13: 0.0
-Dr. 34: ~-0.54
So we see here that Dr. 34 is much less likely to recall a patient for further screening, holding all other factors constant, than Dr. 13. Dr. 66 and Dr. 89 are both much more likely to recall a patient than Dr. 13. Dr. 95 is almost indistinguishable from Dr. 13 in terms of conservativeness. 

```{r,include=FALSE}
##Question 2
# Libraries, idk which ones we really need??
library(tidyverse)
library(foreach)
library(FNN)

# Reading in the raw data from online
brca <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/brca.csv", header=TRUE)

# Overall cancer rate + defining variables
n=nrow(brca)
cancer_rate <- sum(brca$cancer==1)/n #the total rate of breast cancer
y=brca$cancer
threshold = 0.1 #threshold for prediction models, NEED TO FIND GOOD ONE
rmse = function(y,yhat) {
  sqrt(mean((y - yhat)^2))
}

# creating the train test split

coef_df <- data.frame()

for (i in 1:5000){
  n_train <- round(0.8*n)
  n_test <- n - n_train
  train_pats <-sample.int(n,n_train,replace=FALSE)
  test_pats <- setdiff(1:n, train_pats)
  brca_train <- brca[train_pats,]
  brca_test <- brca[test_pats,]
  
  
  #running logit regression for recall on all other variables for train set
  
  logit_drcon = glm(recall ~ ., data=brca_train, family='binomial')
  coef_df <- rbind(coef_df, c(i,coef(logit_drcon)))
  
}

colnames(coef_df) <- c("i", "intercept", "radiologist34", "radiologist66", "radiologist89", "radiologist95",
                       "cancer", "age5059", "age6069", "age70plus", "history", "symptoms", "menopausepostmenoNoHT", "menopausepostmenounknown", "menopausepremeno",
                       "densitydensity2","densitydensity3","densitydensity4")
means_df <- colMeans(coef_df)
means_df

mean(coef_df$radiologist34)
mean(coef_df$radiologist66)
mean(coef_df$radiologist89)
mean(coef_df$radiologist95)

# Out-of-sample accuracy
logit_drcon_test_pred = predict(logit_drcon, brca_test)
logit_drcon_test_yhat = ifelse(logit_drcon_test_pred > threshold, 1, 0) #NEED TO ISOLATE GOOD THRESHOLD!
logit_drcon_confusion_out = table(y=brca_test$cancer, yhat=logit_drcon_test_yhat)
logit_drcon_test_accuracy = sum(diag(logit_drcon_confusion_out))/sum(logit_drcon_confusion_out) #checking accuracy


# Question 2: Are doctors appropriately weighing risk factors?

#first lets run a logit regression of cancer on all risk factors, including recall to get some 
#baseline effects


logit_all = glm(cancer ~ . , data=brca_train, family="binomial")
logit_all

#as we can see, pretty much everything seems to be having some effect, other than maybe being pre-menopausal


# Consider a logit regression of cancer on just recall
#first reset our threshold so that our logit model will have some predictions of cancer = 1

threshold = -2.5 #threshold for prediction model

# now creating a new dataframe so that we can store our results of replicated test train splits
# then we are looping 5000 times a train/test split, storing prediction accuracies, then displaying the mean
coef_recall <- data.frame()

for (i in 1:5000){
  n_train <- round(0.8*n)
  n_test <- n - n_train
  train_pats <-sample.int(n,n_train,replace=FALSE)
  test_pats <- setdiff(1:n, train_pats)
  brca_train <- brca[train_pats,]
  brca_test <- brca[test_pats,]
  
  # logit model
  
  logit_recall = glm(cancer ~ recall, data=brca_train, family="binomial")
  logit_recall
  
  #out of sample accuracy
  
  logit_recall_test_pred = predict(logit_recall, brca_test)
  logit_recall_test_yhat = ifelse(logit_recall_test_pred > threshold, 1, 0) #NEED TO ISOLATE GOOD THRESHOLD!
  logit_recall_confusion_out = table(y=brca_test$cancer, yhat=logit_recall_test_yhat)
  logit_recall_confusion_out
  logit_recall_test_accuracy = sum(diag(logit_recall_confusion_out))/sum(logit_recall_confusion_out) #checking accuracy
  logit_recall_test_accuracy
  
  coef_recall <- rbind(coef_recall, c(i, coef(logit_recall), logit_recall_test_accuracy))
}

colnames(coef_recall) <- c("i", "intercept", "recall", "accuracy")
means_recall <- colMeans(coef_recall)
means_recall


#this gives us a baseline result of roughly 85.72% accuracy

# now that we have a baseline, we can start to look at logit regressions of cancer on recall + 
# other individual risk factors
# first we'll look at density, since it seemed fairly important in the first model
# looping this model 5000 times, similarly to above

coef_density <- data.frame()

for (i in 1:5000){
  n_train <- round(0.8*n)
  n_test <- n - n_train
  train_pats <-sample.int(n,n_train,replace=FALSE)
  test_pats <- setdiff(1:n, train_pats)
  brca_train <- brca[train_pats,]
  brca_test <- brca[test_pats,]
  
  
  logit_plus_density = glm(cancer ~ recall + density, data=brca_train, family='binomial')
  
  #out of sample accuracy
  
  logit_plus_density_test_pred = predict(logit_plus_density, brca_test)
  logit_plus_density_test_yhat = ifelse(logit_plus_density_test_pred > threshold, 1, 0) #NEED TO ISOLATE GOOD THRESHOLD!
  logit_plus_density_confusion_out = table(y=brca_test$cancer, yhat=logit_plus_density_test_yhat)
  logit_plus_density_confusion_out
  logit_plus_density_test_accuracy = sum(diag(logit_plus_density_confusion_out))/sum(logit_plus_density_confusion_out) #checking accuracy
  logit_plus_density_test_accuracy
  
  
  coef_density <- rbind(coef_density, c(i, coef(logit_plus_density), logit_plus_density_test_accuracy))
  
  
  
}

colnames(coef_density) <- c("i", "intercept", "recall", "density1", "density2", "density3", "accuracy")
means_density <- colMeans(coef_density)
means_density

#it seems that dr.'s do a pretty good job of taking density into account, which makes sense becuase it is 
# a very powerful indicator of cancer

#let's try menopause
#another train/test split

coef_meno <- data.frame()

for (i in 1:5000){
  n_train <- round(0.8*n)
  n_test <- n - n_train
  train_pats <-sample.int(n,n_train,replace=FALSE)
  test_pats <- setdiff(1:n, train_pats)
  brca_train <- brca[train_pats,]
  brca_test <- brca[test_pats,]
  
  
  logit_plus_meno = glm(cancer ~ recall + menopause, data=brca_train, family='binomial')
  
  #out of sample accuracy
  
  logit_plus_meno_test_pred = predict(logit_plus_meno, brca_test)
  logit_plus_meno_test_yhat = ifelse(logit_plus_meno_test_pred > threshold, 1, 0) #NEED TO ISOLATE GOOD THRESHOLD!
  logit_plus_meno_confusion_out = table(y=brca_test$cancer, yhat=logit_plus_meno_test_yhat)
  logit_plus_meno_confusion_out
  logit_plus_meno_test_accuracy = sum(diag(logit_plus_meno_confusion_out))/sum(logit_plus_meno_confusion_out) #checking accuracy
  logit_plus_meno_test_accuracy
  
  
  coef_meno <- rbind(coef_meno, c(i, coef(logit_plus_meno), logit_plus_meno_test_accuracy))
  
  
  
}

colnames(coef_meno) <- c("i", "intercept", "recall", "menopausepostmenoNoHT", "menopausepostmenounknown", "menopausepremeno", "accuracy")
means_meno <- colMeans(coef_meno)
means_meno

#menopause seems to be accounted for pretty well, lets try age

# same loop as before but with age
coef_age <- data.frame()

for (i in 1:5000){
  n_train <- round(0.8*n)
  n_test <- n - n_train
  train_pats <-sample.int(n,n_train,replace=FALSE)
  test_pats <- setdiff(1:n, train_pats)
  brca_train <- brca[train_pats,]
  brca_test <- brca[test_pats,]
  
  logit_plus_age = glm(cancer ~ recall + age, data=brca_train, family="binomial")
  logit_plus_age
  
  
  
  #out of sample accuracy
  
  logit_plus_age_test_pred = predict(logit_plus_age, brca_test)
  logit_plus_age_test_yhat = ifelse(logit_plus_age_test_pred > threshold, 1, 0) #NEED TO ISOLATE GOOD THRESHOLD!
  logit_plus_age_confusion_out = table(y=brca_test$cancer, yhat=logit_plus_age_test_yhat)
  logit_plus_age_confusion_out
  logit_plus_age_test_accuracy = sum(diag(logit_plus_age_confusion_out))/sum(logit_plus_age_confusion_out) #checking accuracy
  logit_plus_age_test_accuracy
  
  coef_age <- rbind(coef_age, c(i, coef(logit_plus_age), logit_plus_age_test_accuracy))
  }

colnames(coef_age) <- c("i", "intercept", "recall", "40s", "50s", "60s", "accuracy")
means_age <- colMeans(coef_age)
means_age


#age also seems to be accounted for, lets try family history  

coef_fhist <- data.frame()

for (i in 1:5000){
  n_train <- round(0.8*n)
  n_test <- n - n_train
  train_pats <-sample.int(n,n_train,replace=FALSE)
  test_pats <- setdiff(1:n, train_pats)
  brca_train <- brca[train_pats,]
  brca_test <- brca[test_pats,]
  
  logit_plus_fhist = glm(cancer ~ recall + history, data=brca_train, family="binomial")
  logit_plus_fhist
  
  #out of sample accuracy
  
  logit_plus_fhist_test_pred = predict(logit_plus_fhist, brca_test)
  logit_plus_fhist_test_yhat = ifelse(logit_plus_fhist_test_pred > threshold, 1, 0) #NEED TO ISOLATE GOOD THRESHOLD!
  logit_plus_fhist_confusion_out = table(y=brca_test$cancer, yhat=logit_plus_fhist_test_yhat)
  logit_plus_fhist_confusion_out
  logit_plus_fhist_test_accuracy = sum(diag(logit_plus_fhist_confusion_out))/sum(logit_plus_fhist_confusion_out) #checking accuracy
  logit_plus_fhist_test_accuracy
  
  coef_fhist <- rbind(coef_fhist, c(i, coef(logit_plus_fhist), logit_plus_fhist_test_accuracy))
  
}

colnames(coef_fhist) <- c("i", "intercept", "recall", "fhist", "accuracy")
means_fhist <- colMeans(coef_fhist)
means_fhist

#it looks like dr.s are also really good at taking into account history
#lets examine mamm

coef_symp <- data.frame()

for (i in 1:5000){
  n_train <- round(0.8*n)
  n_test <- n - n_train
  train_pats <-sample.int(n,n_train,replace=FALSE)
  test_pats <- setdiff(1:n, train_pats)
  brca_train <- brca[train_pats,]
  brca_test <- brca[test_pats,]
  
  logit_plus_symp = glm(cancer ~ recall + history, data=brca_train, family="binomial")
  logit_plus_symp
  
  #out of sample accuracy
  
  logit_plus_symp_test_pred = predict(logit_plus_symp, brca_test)
  logit_plus_symp_test_yhat = ifelse(logit_plus_symp_test_pred > threshold, 1, 0) #NEED TO ISOLATE GOOD THRESHOLD!
  logit_plus_symp_confusion_out = table(y=brca_test$cancer, yhat=logit_plus_symp_test_yhat)
  logit_plus_symp_confusion_out
  logit_plus_symp_test_accuracy = sum(diag(logit_plus_symp_confusion_out))/sum(logit_plus_symp_confusion_out) #checking accuracy
  logit_plus_symp_test_accuracy
  
  coef_symp <- rbind(coef_symp, c(i, coef(logit_plus_symp), logit_plus_symp_test_accuracy))
  
}

colnames(coef_symp) <- c("i", "intercept", "recall", "Symptoms", "accuracy")
means_symp <- colMeans(coef_symp)
means_symp

#again, it appears that the dr's, in general, are 
#lets look at a few pairs of risk factors
#here we are looking at a logit regression of cancer on recall + age + density
coef_12 <- data.frame()

for (i in 1:5000){
  n_train <- round(0.8*n)
  n_test <- n - n_train
  train_pats <-sample.int(n,n_train,replace=FALSE)
  test_pats <- setdiff(1:n, train_pats)
  brca_train <- brca[train_pats,]
  brca_test <- brca[test_pats,]
  
  logit_plus_12 = glm(cancer ~ recall + age + density, data=brca_train, family="binomial")
  logit_plus_12
  
  #out of sample accuracy
  
  logit_plus_12_test_pred = predict(logit_plus_12, brca_test)
  logit_plus_12_test_yhat = ifelse(logit_plus_12_test_pred > threshold, 1, 0) #NEED TO ISOLATE GOOD THRESHOLD!
  logit_plus_12_confusion_out = table(y=brca_test$cancer, yhat=logit_plus_12_test_yhat)
  logit_plus_12_confusion_out
  logit_plus_12_test_accuracy = sum(diag(logit_plus_12_confusion_out))/sum(logit_plus_12_confusion_out) #checking accuracy
  logit_plus_12_test_accuracy
  
  coef_12 <- rbind(coef_12, c(i, coef(logit_plus_12), logit_plus_12_test_accuracy))
  
  
  
}

colnames(coef_12) <- c("i", "intercept", "recall", "40", "50", "60", "density1", "density2", "density3", "accuracy")
means_12 <- colMeans(coef_12)
means_12

#this actualy appears to be a more accurate model, suggesting that, while individually these risk
#factors are not significant, they are jointly significant.

#here lets look at the means for all of the different factors, as well as the accuracy rates for each model

means_recall
means_age
means_meno
means_symp
means_fhist
means_12
```

```{r, echo=FALSE}
mean(coef_recall$accuracy)
mean(coef_age$accuracy)
mean(coef_fhist$accuracy)
mean(coef_symp$accuracy)
mean(coef_meno$accuracy)
mean(coef_density$accuracy)
mean(coef_12$accuracy)
```

For the second question, we first wanted to gain a basic understanding of the way all of the factors have on cancer outcomes, so we ran a standard logit regression of cancer on all of the variables. This allowed us to identify particular variables of interest in later analysis.
Next we wanted to run a baseline logit model of cancer on recall. To do this we produced a new train test split of 80/20. We then ran the regression, and tested the coefficients we gained from this regression. Then we stored the accuracy rate and looped back through, with a new train/test split 5000 times. At the end , we computed and accuracy rate of approximately 85.72%.
Once we had a baseline model we were happy with, we started running regressions of cancer on recall and one additional variable for regressor on which we had information, looping each of them 5000 times and averaging their accuracy, yielding the following results
-Recall + density: ~85.74%
-Recall + menopause: ~85.75%
-Recall + age: ~85.68%
-Recall + history: ~85.74%
-Recall + symptoms: ~ 85.54%
Since these are all so close to the accuracy rate of the baseline model, we are confident that the Dr.'s, in general, are doing a very good job of incorporating all available information into their decisions to recall patients for further screening. However, we did want to explore a few more options, specifically, we wanted to see if, despite being individually insignificant, of a pair of regressors might be jointly significant. We were able to find a non-negligible improvement in accuracy when we ran a logit regression of cancer on recall, age and density. Recording an accuracy rate of roughly 86.05%. However, This still only translates to roughly 3 additional correct recalls per 1000 patients, so it may not be worth retraining your on staff doctors in new recall decision techniques. If you do decide to implement new protocols, I would suggest a system in which patients are called in if they meet either set of criteria for recall, simply because the benefits of early detection generally outweigh the concerns of unnecessary recalls. This system should stay in place until it has been verified and can outperform current protocols.

Question 3

```{r, include=FALSE}
## Question 3
online_news <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/online_news.csv", header=TRUE)
library(FNN)

threshold = 1400

online_news$viral = ifelse(online_news$shares > threshold, 1, 0)

count(online_news, viral)[2,2]/nrow(online_news)


## Objective function
rmse = function(y, yhat) {
  sqrt(mean((y - yhat)^2 ))
}

# Model formula
formula <- as.formula(shares ~ 
                        n_tokens_title + n_tokens_content + num_hrefs + num_self_hrefs + 
                        num_imgs + num_videos + average_token_length + num_keywords +
                        data_channel_is_lifestyle + data_channel_is_entertainment + data_channel_is_bus + 
                        data_channel_is_socmed + data_channel_is_tech + data_channel_is_world + 
                        self_reference_avg_sharess + avg_positive_polarity + avg_negative_polarity)
# OLS
lm = lm(formula, data=online_news)
summary(lm)

variable.names(lm)

# Dataframe of RMSE values
lm_df <- data.frame()

for(i in 1:100){
  # Split into training and testing sets
  n = nrow(online_news)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  online_train = online_news[train_cases,]
  online_test = online_news[test_cases,]
  
  ytrain = online_train$shares
  ytest = online_test$shares
  
  viral_train <- ifelse(ytrain > threshold, 1, 0)
  viral_test <- ifelse(ytest> threshold, 1, 0)
  
  ## Linear model
  lm_train = lm(formula, data=online_train)

  yhat_test = predict(lm_train, online_test)
  
  # In-sample performance
  phat_train_online = predict(lm_train, online_train)
  yhat_train_online = ifelse(phat_train_online > 1400, 1, 0)
  confusion_in = table(y = viral_train, yhat = yhat_train_online)
  
  # In-sample accuracy
  in_acc <- sum(diag(confusion_in))/sum(confusion_in)
  
  ## Out-of-sample prediction
  phat_test_online = predict(lm_train, online_test)
  yhat_test_online = ifelse(phat_test_online > 1400, 1, 0)
  confusion_out = table(y = viral_test, yhat = yhat_test_online)
  
  # Out-of-Sample performance
  out_acc <- sum(diag(confusion_out))/sum(confusion_out)
  
  # Root mean-squared prediction error
  yhat_test = predict(lm_train, online_test)
  rmse_online <- rmse(online_test$shares, yhat_test)

  tpr_lm = confusion_out[2,2]/(confusion_out[2,2] + confusion_out[2,1])
  fpr_lm = confusion_out[1,2]/(confusion_out[1,1] + confusion_out[1,2])
  fdr_lm = confusion_out[1,2]/(confusion_out[2,2] + confusion_out[1,2])
  
  # Root mean-squared prediction error
  lm_df <- rbind(lm_df, c(i, rmse_online, in_acc, out_acc, tpr_lm, fpr_lm, fdr_lm))
} 
colnames(lm_df) <- c("i", "rmse", "in_acc", "out_acc", "tpr_lm", "fpr_lm", "fdr_lm")

lm_train = lm(formula, data=online_train)
phat_test_online = predict(lm_train, online_test)
yhat_test_online = ifelse(phat_test_online > 1400, 1, 0)
confusion_out_lm1 = table(y = viral_test, yhat = yhat_test_online)

rmse_lm1 <- mean(lm_df$rmse)
out_acc1 <- mean(lm_df$out_acc)
tpr_acc1 <- mean(lm_df$tpr_lm)
fpr_acc1 <- mean(lm_df$fpr_lm)
fdr_acc1 <- mean(lm_df$fdr_lm)

########### KNN ############

knn_df <- data.frame()

# select a training set
n = nrow(online_news)
n_train = round(0.8*n)
n_test = n - n_train

strt<-Sys.time()

for(i in 2:100){
  cat("k-iteration: [", i, "]\n", sep = "")
  for(j in 1:15) {
    train_cases = sample.int(n, n_train, replace=FALSE)
    test_cases = setdiff(1:n, train_cases)
    online_train = online_news[train_cases,]
    online_test = online_news[test_cases,]
    
    # construct the training and test-set feature matrices
    # note the "-1": this says "don't add a column of ones for the intercept"
    Xtrain = model.matrix(~ n_tokens_title + n_tokens_content + num_hrefs + num_self_hrefs + 
                            num_imgs + num_videos + average_token_length + num_keywords +
                            data_channel_is_lifestyle + data_channel_is_entertainment + data_channel_is_bus + 
                            data_channel_is_socmed + data_channel_is_tech + data_channel_is_world + 
                            self_reference_avg_sharess + avg_positive_polarity + avg_negative_polarity
                          - 1, data=online_train)
    Xtest = model.matrix(~ n_tokens_title + n_tokens_content + num_hrefs + num_self_hrefs + 
                           num_imgs + num_videos + average_token_length + num_keywords +
                           data_channel_is_lifestyle + data_channel_is_entertainment + data_channel_is_bus + 
                           data_channel_is_socmed + data_channel_is_tech + data_channel_is_world + 
                           self_reference_avg_sharess + avg_positive_polarity + avg_negative_polarity
                         - 1, data=online_test)
    
    # training and testing set responses
    ytrain = online_train$shares
    ytest = online_test$shares
    
    viral_train <- ifelse(ytrain > threshold, 1, 0)
    viral_test <- ifelse(ytest > threshold, 1, 0)
    
    # now rescale:
    scale_train = apply(Xtrain, 2, sd)  # calculate std dev for each column
    Xtilde_train = scale(Xtrain, scale = scale_train)
    Xtilde_test = scale(Xtest, scale = scale_train)  # use the training set scales!
    
    ## KNN regression model
    knn_model = knn.reg(Xtilde_train, Xtilde_test, ytrain, k=i)
    rmse_knn <- rmse(ytest, knn_model$pred)
    
    ## Out-of-sample prediction
    phat_test_knn = knn_model$pred  # predicted probabilities
    yhat_test_knn = ifelse(phat_test_knn > threshold, 1, 0)
    confusion_out = table(y = viral_test, yhat = yhat_test_knn)
    
    # Out-of-Sample performance
    out_acc <- sum(diag(confusion_out))/sum(confusion_out)
    
    tpr_knn = confusion_out[2,2]/(confusion_out[2,2] + confusion_out[2,1])
    fpr_knn = confusion_out[1,2]/(confusion_out[1,1] + confusion_out[1,2])
    fdr_knn = confusion_out[1,2]/(confusion_out[2,2] + confusion_out[1,2])
    
    ## Store all stastics in df
    knn_df <- rbind(knn_df, c(i, rmse_knn, out_acc, tpr_knn, fpr_knn, fdr_knn))
  }
}
runtime <- cat(Sys.time()-strt)

colnames(knn_df) <- c("knn","rmse", "out_acc", "tpr_knn", "fpr_knn", "fdr_knn")
rmse_df <- aggregate(rmse ~ knn, knn_df, mean)
acc_df <- aggregate(out_acc ~ knn, knn_df, mean)
tpr_df <- aggregate(tpr_knn ~ knn, knn_df, mean)
fpr_df <- aggregate(fpr_knn ~ knn, knn_df, mean)
fdr_df <- aggregate(fdr_knn ~ knn, knn_df, mean)


rmse_df[which.min(rmse_df$rmse),]
knn_df[which.max(acc_df$out_acc),]
knn_df[which.max(tpr_df$tpr_knn),]
knn_df[which.max(fpr_df$fpr_knn),]
knn_df[which.max(fdr_df$fdr_knn),]
rmse_lm

knn_best <- rmse_df[which.min(rmse_df$rmse),]$knn
knn_best 

## Get confusion matrix
knn_model = knn.reg(Xtilde_train, Xtilde_test, ytrain, k=knn_best)
phat_test_knn = knn_model$pred  # predicted probabilities
yhat_test_knn = ifelse(phat_test_knn > threshold, 1, 0)
confusion_out_knn = table(y = viral_test, yhat = yhat_test_knn)
confusion_out_knn

#### Graphs ####

## Below is our graph for RMSE across many values of K
# The Red line represents the RMSE of a linear model, 
# The minimum RMSE of the linear model which is typically around 11,000
# The Green line represnt the minimum RMSE of the KNN model, 
# The minimum K value is usually between 10 and 20 with a RMSE of around 8,000

plot(rmse ~ knn, data = rmse_df, 
     ylim=c(min(rmse_df$rmse) - min(rmse_df$rmse)*0.05, max(rmse_df$rmse) + max(rmse_df$rmse)*0.05), 
     main="RMSE for KNN and Linear Model")
abline(h=rmse_lm1, col = 2)
abline(v=which.min(rmse_df$rmse)+1, col = 3)

plot(out_acc ~ knn, data = acc_df, 
     ylim=c(out_acc1 - out_acc1*0.05, max(acc_df$out_acc) + max(acc_df$out_acc)*0.05), 
     main= "Out-of-Sample Accuracy for KNN and Linear Model")
abline(h=out_acc1, col = 2)
abline(v=which.min(acc_df$out_acc)+1, col = 3)

### Statistics  ###

## Linear Model
# Confusion Matrix
confusion_out_lm1
# RMSE
rmse_lm1
# Out-of-sample Accuracy
out_acc1
# Overall Error Rate
1 - out_acc1
# True Positive Rate 
tpr_acc1
# False Positive Rate
fpr_acc1
# False Discovery Rate
fdr_acc1

## KNN
# Confusion matrix
confusion_out_knn
# RMSE
min(rmse_df$rmse)
# Out-of-sample Accuracy
max(acc_df$out_acc)
# Overall Error Rate 
1-max(acc_df$out_acc)
# True Positive Rate
max(tpr_df$tpr_knn)
# False Positive Rate
max(fpr_df$fpr_knn)
# False Discovery Rate
max(fdr_df$fdr_knn)

########### PART 2 ##############
## Question 3
library(FNN)

## Objective function
rmse = function(y, yhat) {
  sqrt(mean((y - yhat)^2 ))
}

threshold = 1400
online_news$viral = ifelse(online_news$shares > threshold, 1, 0)

formula <- as.formula(viral ~ 
                              n_tokens_title + n_tokens_content + num_hrefs + num_self_hrefs + 
                              num_imgs + num_videos + average_token_length + num_keywords +
                              data_channel_is_lifestyle + data_channel_is_entertainment + data_channel_is_bus + 
                              data_channel_is_socmed + data_channel_is_tech + data_channel_is_world + 
                              self_reference_avg_sharess + avg_positive_polarity + avg_negative_polarity)


threshold = 1400

## Linear Model
 
lm_df2 <- data.frame()

for(i in 1:100){
  # Split into training and testing sets
  n = nrow(online_news)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  online_train = online_news[train_cases,]
  online_test = online_news[test_cases,]
  
  lm_online = lm(formula, data=online_train)
  
  # In-sample performance
  phat_train_online = predict(lm_online, online_train)
  yhat_train_online = ifelse(phat_train_online > 0.5, 1, 0)
  confusion_in = table(y = online_train$viral, yhat = yhat_train_online)
  
  # In-sample accuracy
  in_acc <- sum(diag(confusion_in))/sum(confusion_in)
  
  ## Out-of-sample prediction
  phat_test_online = predict(lm_online, online_test)
  yhat_test_online = ifelse(phat_test_online > 0.5, 1, 0)
  confusion_out = table(y = online_test$viral, yhat = yhat_test_online)
  
  # Out-of-Sample performance
  out_acc <- sum(diag(confusion_out))/sum(confusion_out)
  
  # Root mean-squared prediction error
  yhat_test = predict(lm_online, online_test)
  rmse_online <- rmse(online_test$shares, yhat_test)
  
  tpr_lm2 = confusion_out[2,2]/(confusion_out[2,2] + confusion_out[2,1])
  fpr_lm2 = confusion_out[1,2]/(confusion_out[1,1] + confusion_out[1,2])
  fdr_lm2 = confusion_out[1,2]/(confusion_out[2,2] + confusion_out[1,2])
  
  lm_df2 <- rbind(lm_df2, c(i, rmse_online, in_acc, out_acc, tpr_lm2, fpr_lm2, fdr_lm2))
  
} 
colnames(lm_df2) <- c("i", "rmse", "in_acc", "out_acc", "tpr_lm", "fpr_lm", "fdr_lm")
colMeans(lm_df2)

lm_online2 = lm(formula, data=online_train)
phat_test_online = predict(lm_online2, online_test)
yhat_test_online = ifelse(phat_test_online > 0.5, 1, 0)
confusion_out2 <- table(y = online_test$viral, yhat = yhat_test_online)

rmse_lm2 <- mean(lm_df2$rmse)
out_acc2 <- mean(lm_df2$out_acc)
tpr_acc2 <- mean(lm_df2$tpr_lm)
fpr_acc2 <- mean(lm_df2$fpr_lm)
fdr_acc2 <- mean(lm_df2$fdr_lm)

###### LOGIT MODEL

logit_df <- data.frame()

for(i in 1:100){
  # Split into training and testing sets
  n = nrow(online_news)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  online_train = online_news[train_cases,]
  online_test = online_news[test_cases,]
  
  logit_online = glm(formula, data=online_train, family='binomial')
  
  # In-sample performance
  phat_train_online = predict(logit_online, online_train)
  yhat_train_online = ifelse(phat_train_online > 0.5, 1, 0)
  confusion_in = table(y = online_train$viral, yhat = yhat_train_online)

  # In-sample accuracy
  in_acc <- sum(diag(confusion_in))/sum(confusion_in)
  
  ## Out-of-sample prediction
  phat_test_online = predict(logit_online, online_test)
  yhat_test_online = ifelse(phat_test_online > 0.5, 1, 0)
  confusion_out = table(y = online_test$viral, yhat = yhat_test_online)

  # Out-of-Sample performance
  out_acc <- sum(diag(confusion_out))/sum(confusion_out)
  
  # Root mean-squared prediction error
  yhat_test = predict(logit_online, online_test)
  rmse_online <- rmse(online_test$shares, yhat_test)
  
  tpr_logit = confusion_out[2,2]/(confusion_out[2,2] + confusion_out[2,1])
  fpr_logit = confusion_out[1,2]/(confusion_out[1,1] + confusion_out[1,2])
  fdr_logit = confusion_out[1,2]/(confusion_out[2,2] + confusion_out[1,2])
  
  logit_df <- rbind(logit_df, c(i, rmse_online, in_acc, out_acc, tpr_logit, fpr_logit, fdr_logit))
  
} 
colnames(logit_df) <- c("i", "rmse", "in_acc", "out_acc", "tpr_logit", "fpr_logit", "fdr_logit")
colMeans(logit_df)

logit_online2 = glm(formula, data=online_train, family='binomial')
# In-sample performance
phat_test_online = predict(logit_online, online_test)
yhat_test_online = ifelse(phat_test_online > 0.5, 1, 0)
confusion_out_logit = table(y = online_test$viral, yhat = yhat_test_online)

rmse_logit <- mean(logit_df$rmse)
out_logit <- mean(logit_df$out_acc)
tpr_logit <- mean(logit_df$tpr_logit)
fpr_logit <- mean(logit_df$fpr_logit)
fdr_logit <- mean(logit_df$fdr_logit)


### Statistics  ###

## Linear Model
# Confusion Matrix
confusion_out2
# RMSE
rmse_lm2
# Out-of-sample Accuracy
out_acc2
# Overall Error Rate
1 - out_acc2
# True Positive Rate 
tpr_acc2
# False Positive Rate
fpr_acc2
# False Discovery Rate
fdr_acc2

## Logit Model
# Confusion Matrix
confusion_out_logit
# RMSE
rmse_logit
# Out-of-sample Accuracy
out_logit
# Overall Error Rate
1 - out_logit
# True Positive Rate 
tpr_logit
# False Positive Rate
fpr_logit
# False Discovery Rate
fdr_logit

# Both models are not great at predicting whether an article will go viral or not. 
# There is a bias-variance tradeoff between the two methods.
```

Below is our graph for RMSE across many values of K. The Red line represents the RMSE of a linear model, the minimum RMSE of the linear model which is typically around 11,000. The Green line represnt the minimum RMSE of the KNN model. The minimum K value is usually between 10 and 20 with a RMSE of around 8,000.
```{r, echo=FALSE}
plot(rmse ~ knn, data = rmse_df, 
     ylim=c(min(rmse_df$rmse) - min(rmse_df$rmse)*0.05, max(rmse_df$rmse) + max(rmse_df$rmse)*0.05), 
     main="RMSE for KNN and Linear Model")
abline(h=rmse_lm1, col = 2)
abline(v=which.min(rmse_df$rmse)+1, col = 3)

plot(out_acc ~ knn, data = acc_df, 
     ylim=c(out_acc1 - out_acc1*0.05, max(acc_df$out_acc) + max(acc_df$out_acc)*0.05), 
     main= "Out-of-Sample Accuracy for KNN and Linear Model")
abline(h=out_acc1, col = 2)
abline(v=which.min(acc_df$out_acc)+1, col = 3)

### Statistics  ###

## Linear Model
# Confusion Matrix
confusion_out_lm1
# RMSE
rmse_lm1
# Out-of-sample Accuracy
out_acc1
# Overall Error Rate
1 - out_acc1
# True Positive Rate 
tpr_acc1
# False Positive Rate
fpr_acc1
# False Discovery Rate
fdr_acc1

## KNN
# Confusion matrix
confusion_out_knn
# RMSE
min(rmse_df$rmse)
# Out-of-sample Accuracy
max(acc_df$out_acc)
# Overall Error Rate 
1-max(acc_df$out_acc)
# True Positive Rate
max(tpr_df$tpr_knn)
# False Positive Rate
max(fpr_df$fpr_knn)
# False Discovery Rate
max(fdr_df$fdr_knn)
```

```{r, echo=FALSE}
## Linear Model
# Confusion Matrix
confusion_out2
# RMSE
rmse_lm2
# Out-of-sample Accuracy
out_acc2
# Overall Error Rate
1 - out_acc2
# True Positive Rate 
tpr_acc2
# False Positive Rate
fpr_acc2
# False Discovery Rate
fdr_acc2

## Logit Model
# Confusion Matrix
confusion_out_logit
# RMSE
rmse_logit
# Out-of-sample Accuracy
out_logit
# Overall Error Rate
1 - out_logit
# True Positive Rate 
tpr_logit
# False Positive Rate
fpr_logit
# False Discovery Rate
fdr_logit
```

Both models are not great at predicting whether an article will go viral or not. There is a bias-variance tradeoff between the two methods.